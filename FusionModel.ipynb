{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bbc063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:47:45,213 - INFO - Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 16:47:45,214 - INFO - Loading tokenizer...\n",
      "2025-04-30 16:47:45,416 - INFO - Loading datasets...\n",
      "2025-04-30 16:47:46,776 - INFO - Loaded Train: 63639, Val: 7981, Test: 7963\n",
      "2025-04-30 16:47:46,873 - INFO - Loaded image model weights\n",
      "2025-04-30 16:47:46,878 - INFO - Loaded text model weights\n",
      "2025-04-30 16:47:47,102 - INFO - Training fusion model...\n",
      "Epoch 1 Training: 100%|██████████| 3978/3978 [18:51<00:00,  3.52it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:16<00:00,  3.65it/s]\n",
      "2025-04-30 17:08:55,285 - INFO - Epoch 1/7, Train Loss: 0.1617, Val F1 Macro: 0.4989, Val Accuracy: 0.2191\n",
      "Epoch 2 Training: 100%|██████████| 3978/3978 [18:10<00:00,  3.65it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:15<00:00,  3.69it/s]\n",
      "2025-04-30 17:29:20,942 - INFO - Epoch 2/7, Train Loss: 0.1503, Val F1 Macro: 0.4845, Val Accuracy: 0.2322\n",
      "Epoch 3 Training: 100%|██████████| 3978/3978 [18:10<00:00,  3.65it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:15<00:00,  3.68it/s]\n",
      "2025-04-30 17:49:47,198 - INFO - Epoch 3/7, Train Loss: 0.1480, Val F1 Macro: 0.4927, Val Accuracy: 0.2272\n",
      "Epoch 4 Training: 100%|██████████| 3978/3978 [18:09<00:00,  3.65it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:15<00:00,  3.68it/s]\n",
      "2025-04-30 18:10:12,973 - INFO - Epoch 4/7, Train Loss: 0.1469, Val F1 Macro: 0.5023, Val Accuracy: 0.2263\n",
      "Epoch 5 Training: 100%|██████████| 3978/3978 [22:00<00:00,  3.01it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:30<00:00,  3.31it/s]\n",
      "2025-04-30 18:34:44,362 - INFO - Epoch 5/7, Train Loss: 0.1456, Val F1 Macro: 0.4773, Val Accuracy: 0.2332\n",
      "Epoch 6 Training: 100%|██████████| 3978/3978 [21:05<00:00,  3.14it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:37<00:00,  3.16it/s]\n",
      "2025-04-30 18:58:28,233 - INFO - Epoch 6/7, Train Loss: 0.1449, Val F1 Macro: 0.4922, Val Accuracy: 0.2262\n",
      "Epoch 7 Training: 100%|██████████| 3978/3978 [20:46<00:00,  3.19it/s]\n",
      "Evaluation: 100%|██████████| 499/499 [02:57<00:00,  2.82it/s]\n",
      "2025-04-30 19:22:11,581 - INFO - Epoch 7/7, Train Loss: 0.1444, Val F1 Macro: 0.4944, Val Accuracy: 0.2228\n",
      "2025-04-30 19:22:11,583 - INFO - Early stopping triggered\n",
      "2025-04-30 19:22:11,770 - INFO - Best validation F1 Macro: 0.5023\n",
      "2025-04-30 19:22:11,771 - INFO - Evaluating on test set...\n",
      "Evaluation: 100%|██████████| 498/498 [02:28<00:00,  3.36it/s]\n",
      "2025-04-30 19:24:40,321 - INFO - Fusion Model Evaluation:\n",
      "2025-04-30 19:24:40,323 - INFO - F1 Macro: 0.5006, F1 Micro: 0.5551, Accuracy: 0.2314, Precision: 0.5006, Recall: 0.5006\n",
      "2025-04-30 19:24:40,327 - INFO - Action F1: 0.5191\n",
      "2025-04-30 19:24:40,332 - INFO - Adventure F1: 0.3407\n",
      "2025-04-30 19:24:40,336 - INFO - Animation F1: 0.7169\n",
      "2025-04-30 19:24:40,340 - INFO - Comedy F1: 0.6086\n",
      "2025-04-30 19:24:40,345 - INFO - Crime F1: 0.4821\n",
      "2025-04-30 19:24:40,350 - INFO - Documentary F1: 0.6773\n",
      "2025-04-30 19:24:40,355 - INFO - Drama F1: 0.6588\n",
      "2025-04-30 19:24:40,360 - INFO - Family F1: 0.5151\n",
      "2025-04-30 19:24:40,364 - INFO - Fantasy F1: 0.3353\n",
      "2025-04-30 19:24:40,369 - INFO - History F1: 0.2108\n",
      "2025-04-30 19:24:40,373 - INFO - Horror F1: 0.6458\n",
      "2025-04-30 19:24:40,378 - INFO - Music F1: 0.6895\n",
      "2025-04-30 19:24:40,382 - INFO - Mystery F1: 0.3326\n",
      "2025-04-30 19:24:40,386 - INFO - Romance F1: 0.3755\n",
      "2025-04-30 19:24:40,390 - INFO - Science Fiction F1: 0.6004\n",
      "2025-04-30 19:24:40,394 - INFO - TV Movie F1: 0.1464\n",
      "2025-04-30 19:24:40,399 - INFO - Thriller F1: 0.4671\n",
      "2025-04-30 19:24:40,404 - INFO - War F1: 0.4811\n",
      "2025-04-30 19:24:40,408 - INFO - Western F1: 0.7080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "logging.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "all_genres = [\"Action\", \"Adventure\", \"Animation\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\",\n",
    "              \"Family\", \"Fantasy\", \"History\", \"Horror\", \"Music\", \"Mystery\", \"Romance\",\n",
    "              \"Science Fiction\", \"TV Movie\", \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "\n",
    "logging.info(\"Loading tokenizer...\")\n",
    "with open(\"tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_config = json.load(f)\n",
    "tokenizer = tokenizer_from_json(tokenizer_config)\n",
    "max_len = 100\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "train_df = pd.read_json(\"train_fixed_updated.json\")\n",
    "val_df = pd.read_json(\"val_fixed_updated.json\")\n",
    "test_df = pd.read_json(\"test_fixed_updated.json\")\n",
    "logging.info(f\"Loaded Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "\n",
    "class ImageMovieGenreClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        features = self.resnet.conv1(x)\n",
    "        features = self.resnet.bn1(features)\n",
    "        features = self.resnet.relu(features)\n",
    "        features = self.resnet.maxpool(features)\n",
    "        features = self.resnet.layer1(features)\n",
    "        features = self.resnet.layer2(features)\n",
    "        features = self.resnet.layer3(features)\n",
    "        features = self.resnet.layer4(features)\n",
    "        features = self.resnet.avgpool(features)\n",
    "        features = torch.flatten(features, 1)\n",
    "        outputs = self.resnet.fc(features)\n",
    "        return self.sigmoid(outputs), features\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, lstm_output):\n",
    "        energy = self.tanh(self.attn(lstm_output))\n",
    "        attention = self.softmax(torch.matmul(energy, self.v.unsqueeze(1)).squeeze(2))\n",
    "        context = torch.bmm(attention.unsqueeze(1), lstm_output).squeeze(1)\n",
    "        return context\n",
    "\n",
    "\n",
    "class AttentionMovieGenreClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size=10000, embedding_dim=128, hidden_dim=128, output_dim=19):\n",
    "        super(AttentionMovieGenreClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True, dropout=0.0, bidirectional=True)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
    "        context = self.attention(lstm_output)\n",
    "        dense = self.dropout(context)\n",
    "        output = self.fc(dense)\n",
    "        output = self.sigmoid(output / self.temperature)\n",
    "        return output, context\n",
    "\n",
    "class FusionModel(nn.Module):\n",
    "    def __init__(self, image_in_features=512, text_in_features=256, num_classes=19):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(image_in_features + text_in_features, 256)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, image_features, text_features):\n",
    "        x = torch.cat((image_features, text_features), dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, tokenizer, max_len=100):\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.df.iloc[idx][\"poster_path\"].lstrip(\"/\"))\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error loading image {img_path}: {e}\")\n",
    "            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
    "        image = self.transform(image)\n",
    "        text = self.df.iloc[idx][\"processed_overview\"] or \"\"\n",
    "        sequence = self.tokenizer.texts_to_sequences([text])[0]\n",
    "        padded = pad_sequences([sequence], maxlen=self.max_len, padding=\"post\", truncating=\"post\")\n",
    "        text_tensor = torch.tensor(padded, dtype=torch.long).squeeze()\n",
    "        labels = np.array(self.df.iloc[idx][\"genre_labels\"], dtype=np.float32)\n",
    "        return image, text_tensor, torch.tensor(labels)\n",
    "\n",
    "\n",
    "def evaluate_model(image_model, text_model, fusion_model, loader):\n",
    "    image_model.eval()\n",
    "    text_model.eval()\n",
    "    fusion_model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in tqdm(loader, desc=\"Evaluation\"):\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            _, image_features = image_model(images)\n",
    "            _, text_features = text_model(texts)\n",
    "            outputs = fusion_model(image_features, text_features)\n",
    "            preds.append(outputs.cpu().numpy())\n",
    "            true.append(labels.cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    true = np.concatenate(true)\n",
    "    preds_binary = (preds > 0.5).astype(int)\n",
    "    f1_macro = f1_score(true, preds_binary, average=\"macro\")\n",
    "    f1_micro = f1_score(true, preds_binary, average=\"micro\")\n",
    "    accuracy = accuracy_score(true, preds_binary)\n",
    "    precision = np.mean([f1_score(true[:, i], preds_binary[:, i], average=\"binary\", zero_division=0) for i in range(len(all_genres))])\n",
    "    recall = np.mean([f1_score(true[:, i], preds_binary[:, i], average=\"binary\", zero_division=0) for i in range(len(all_genres))])\n",
    "    return preds_binary, true, {\"f1_macro\": f1_macro, \"f1_micro\": f1_micro, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "\n",
    "def train_fusion_model(image_model, text_model, fusion_model, train_loader, val_loader, epochs=7):\n",
    "    optimizer = torch.optim.AdamW(fusion_model.parameters(), lr=0.001)\n",
    "    class_weights = torch.ones(19).to(device)\n",
    "    class_weights[15] = 2.0\n",
    "    class_weights[9] = 2.0  \n",
    "    criterion = nn.BCELoss(weight=class_weights)\n",
    "    best_f1 = 0\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "    metrics_history = {\"train_loss\": [], \"val_f1_macro\": [], \"val_accuracy\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        fusion_model.train()\n",
    "        train_loss = 0\n",
    "        for images, texts, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            images = images.to(device)\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                _, image_features = image_model(images)\n",
    "                _, text_features = text_model(texts)\n",
    "            outputs = fusion_model(image_features, text_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        metrics_history[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        val_preds_binary, val_true, metrics = evaluate_model(image_model, text_model, fusion_model, val_loader)\n",
    "        metrics_history[\"val_f1_macro\"].append(metrics[\"f1_macro\"])\n",
    "        metrics_history[\"val_accuracy\"].append(metrics[\"accuracy\"])\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "                     f\"Val F1 Macro: {metrics['f1_macro']:.4f}, Val Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        \n",
    "        if metrics[\"f1_macro\"] > best_f1:\n",
    "            best_f1 = metrics[\"f1_macro\"]\n",
    "            torch.save(fusion_model.state_dict(), \"best_fusion_model.pth\")\n",
    "            torch.save(fusion_model.state_dict(), f\"fusion_model_epoch_{epoch+1}.pth\")\n",
    "            np.save(\"val_fusion_preds.npy\", val_preds_binary)\n",
    "            np.save(\"val_fusion_true.npy\", val_true)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logging.info(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "   \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metrics_history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"loss_curves_fusion.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metrics_history[\"val_f1_macro\"], label=\"Val F1 Macro\")\n",
    "    plt.title(\"Validation F1 Macro\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"F1 Macro\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"f1_scores_fusion.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(metrics_history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "    plt.title(\"Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"accuracy_scores_fusion.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return best_f1\n",
    "\n",
    "\n",
    "def final_evaluation(image_model, text_model, fusion_model, test_loader):\n",
    "    preds_binary, true, metrics = evaluate_model(image_model, text_model, fusion_model, test_loader)\n",
    "    logging.info(\"Fusion Model Evaluation:\")\n",
    "    logging.info(f\"F1 Macro: {metrics['f1_macro']:.4f}, F1 Micro: {metrics['f1_micro']:.4f}, \"\n",
    "                 f\"Accuracy: {metrics['accuracy']:.4f}, Precision: {metrics['precision']:.4f}, \"\n",
    "                 f\"Recall: {metrics['recall']:.4f}\")\n",
    "    for i, genre in enumerate(all_genres):\n",
    "        f1 = f1_score(true[:, i], preds_binary[:, i], average=\"binary\", zero_division=0)\n",
    "        logging.info(f\"{genre} F1: {f1:.4f}\")\n",
    "    return preds_binary, true, metrics\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   \n",
    "    image_model = ImageMovieGenreClassifier().to(device)\n",
    "    text_model = AttentionMovieGenreClassifier().to(device)\n",
    "    fusion_model = FusionModel().to(device)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        image_model.load_state_dict(torch.load(\"best_image_model.pth\", map_location=device))\n",
    "        logging.info(\"Loaded image model weights\")\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"Image model weights not found. Initialize with random weights.\")\n",
    "    \n",
    "    try:\n",
    "        text_model.load_state_dict(torch.load(\"best_text_model.pth\", map_location=device))\n",
    "        logging.info(\"Loaded text model weights\")\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"Text model weights not found. Initialize with random weights.\")\n",
    "    \n",
    "    \n",
    "    train_dataset = FusionDataset(train_df, \"images/\", tokenizer, max_len)\n",
    "    val_dataset = FusionDataset(val_df, \"images/\", tokenizer, max_len)\n",
    "    test_dataset = FusionDataset(test_df, \"images/\", tokenizer, max_len)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, num_workers=0)\n",
    "    \n",
    "   \n",
    "    logging.info(\"Training fusion model...\")\n",
    "    best_f1 = train_fusion_model(image_model, text_model, fusion_model, train_loader, val_loader, epochs=7)\n",
    "    logging.info(f\"Best validation F1 Macro: {best_f1:.4f}\")\n",
    "    \n",
    "    \n",
    "    logging.info(\"Evaluating on test set...\")\n",
    "    fusion_model.load_state_dict(torch.load(\"best_fusion_model.pth\", map_location=device))\n",
    "    test_preds_binary, test_true, test_metrics = final_evaluation(image_model, text_model, fusion_model, test_loader)\n",
    "    \n",
    "    \n",
    "    np.save(\"test_fusion_preds.npy\", test_preds_binary)\n",
    "    np.save(\"test_fusion_true.npy\", test_true)\n",
    "    with open(\"test_fusion_metrics.json\", \"w\") as f:\n",
    "        json.dump(test_metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
