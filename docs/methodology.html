<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Multi-Modal Movie Genre Classification</title>
        <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&family=Cinzel:wght@700&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="styles.css">
    </head>
<body>
    <nav>
        <a href="index.html">Home</a>
        <a href="dataset.html">Dataset</a>
        <a href="methodology.html">Methodology</a>
        <a href="results.html">Results</a>
        <a href="reproducibility.html">Reproducibility</a>
        <a href="contributions.html">Contributions</a>
        <a href="discussion.html">Discussion</a>
    </nav>
    <div class="container">
        <h1>Methodology</h1>
        <h2>Text Processing</h2>
        <p>The text processing pipeline features an LSTM network with attention, which is used to extract semantic features from the movie plot synopses. The synopses are first tokenized using the NLTK word tokenizer, with special characters, punctuation, and stopwords removed to minimize noise. Using pre-trained GloVe embeddings of 300 dimensions, tokens are converted into dense vector representations preserving semantic relations. Synopses are padded or truncated up to 100 tokens, an experimentally determined size that strikes a balance between keeping information intact and minimizing computation. The LSTM has hidden layers of 256 units with 2 layers processing the sequence, which learns temporal dependencies describing the advance in the plot. An attention mechanism weighs tokens in the sentence, like "mysterious" in Mystery, and produces a 256-dimensional feature vector. For rare genre categories like TV Movie, class-specific attention weights were fine-tuned, improving F1 scores, in the case of the text-only model, for instance, F1 of Mystery from 0.2800 to 0.2974. Dropout with a probability of 0.3 and L2 regularization of 0.01 were used to hamper overfitting, which was very much an issue for the oversampled training set of 74,000 samples.</p>
        <h2>Image Processing</h2>
        <p>Poster images are fed through a ResNet18 pre-trained model, with weights fine-tuned for genre classification. Images are resized to 224x224 pixels and normalized according to ResNet requirements. Data augmentations help to reinforce poster variation: random rotation and horizontal flip, color jitter, and random crop. The ResNet18 groups 18 layers of convolution for extracting visual features. The last fully-connected layer is replaced by a fully-connected layer with 512 units so as to generate a new 512-dimensional visual feature vector. Fine-tuning was performed so that the last three convolution blocks only were unfrozen. Due to this restriction, some ImageNet weights were adapted to movie posters and led to an F1 Macro increase from 0.3200 to 0.3567. A 0.5 dropout and batch normalization were then used to stabilize the training and avoid overfitting, especially popular with fewer training samples (e.g., TV Movie: 0.2342). Visual feature ambiguity (i.e., TV Movie posters somewhat resembling Drama) was a problem and was alleviated by augmentations and weighing classes.</p>
        <h2>Fusion Technique</h2>
        <p>The fusion model concatenates text (256D) and image (512D) features to create a joint 768D representation. This vector is used as input for two fully connected layers (768→256→19) with ReLU activation, batch normalization, and dropout with a rate of 0.4. The sigmoid layer outputs probabilities for the 19 genres in a multi-label classification setting. To handle class imbalance, the binary cross-entropy loss was weighted for rare genres (TV Movie: 2.0, History: 2.0), which increased the History F1 score from 0.1860 to 0.2108. Other fusion models, such as weighted sum and cross-modal attention, were attempted but yielded marginal improvements (F1 Macro: 0.4980 vs. 0.5006) for much higher complexity. It was decided to employ simple concatenation, which lets the model learn complementary signals.</p>
        <h2>Hyperparameters</h2>
        <p>Training of the model took place with the AdamW optimizer of learning rate 0.001 and weight decay of 0.01, while a cosine annealing scheduler scaled it down to 0.0001 in 10 epochs. The batch size was 16, limited by CPU memory, allowing repetition. The oversampled training set (74,000 samples, padded shape: 74000x100, labels: 74000x19) underwent random oversampling to assist some rare genres, raising TV Movie samples by 1.5 times. Training continued for 10 epochs with early stopping (patience = 3) within the evaluation of validation F1 Macro, ceasing at epoch 9 for the fusion model (Val F1 Macro: 0.4345). Class weighting and focal loss (TV Movie, History = 2.0; alpha = 0.25; gamma = 2) were also used to address imbalance.</p>
    </div>
</body>
</html>
