<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Results - Movie Genre Classification</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 0; padding: 20px; }
        nav { background: #333; padding: 10px; }
        nav a { color: white; margin: 0 15px; text-decoration: none; }
        .container { max-width: 800px; margin: auto; }
        h1, h2 { color: #333; }
        img { max-width: 100%; height: auto; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background: #f2f2f2; }
    </style>
</head>
<body>
    <nav>
        <a href="index.html">Home</a>
        <a href="dataset.html">Dataset</a>
        <a href="methodology.html">Methodology</a>
        <a href="results.html">Results</a>
        <a href="reproducibility.html">Reproducibility</a>
        <a href="contributions.html">Contributions</a>
        <a href="discussion.html">Discussion</a>
    </nav>
    <div class="container">
        <h1>Results & Evaluation</h1>
        <h2>Model Performance Comparison</h2>
        <p>The project evaluated three models: a text-only model (LSTM with attention on plot synopses), an image-only model (ResNet18 on movie posters), and a fusion model (combining both modalities). The table below summarizes their performance on the test set (7,963 samples) using F1 Macro, F1 Micro, and Accuracy metrics. F1 Macro is the primary metric due to its robustness for multi-label classification with imbalanced genres.</p>
        <table>
            <tr><th>Model</th><th>F1 Macro</th><th>F1 Micro</th><th>Accuracy</th></tr>
            <tr><td>Text-Only (LSTM)</td><td>0.4958</td><td>0.5365</td><td>0.8861</td></tr>
            <tr><td>Image-Only (ResNet18)</td><td>0.3567</td><td>0.4286</td><td>0.8377</td></tr>
            <tr><td>Fusion (LSTM + ResNet18)</td><td>0.5006</td><td>0.5551</td><td>0.2314</td></tr>
        </table>
        <p>The fusion model achieved the highest F1 Macro (0.5006), meeting the target range of 0.45–0.50 and outperforming the text-only (0.4958) and image-only (0.3567) models. The text-only model excelled due to rich semantic cues in synopses, particularly for genres like Drama (F1: 0.6625) and Music (F1: 0.7183). The image-only model struggled with genres lacking distinct visual markers (e.g., TV Movie: 0.2342, History: 0.1439), but performed well for visually distinct genres like Animation (F1: 0.6636). The fusion model’s improvement demonstrates the value of combining textual and visual signals.</p>
        <h2>Per-Genre F1 Scores</h2>
        <p>The table below compares F1 scores for all 19 genres across the three models, with targets for Mystery (>0.25), History (>0.20), and TV Movie (>0.20).</p>
        <table>
            <tr><th>Genre</th><th>Text-Only F1</th><th>Image-Only F1</th><th>Fusion F1</th></tr>
            <tr><td>Animation</td><td>0.5451</td><td>0.6636</td><td>0.7169</td></tr>
            <tr><td>Western</td><td>0.6500</td><td>0.4077</td><td>0.7080</td></tr>
            <tr><td>Music</td><td>0.7183</td><td>0.2816</td><td>0.6895</td></tr>
            <tr><td>Documentary</td><td>0.7004</td><td>0.3643</td><td>0.6773</td></tr>
            <tr><td>Drama</td><td>0.6625</td><td>0.6262</td><td>0.6588</td></tr>
            <tr><td>Horror</td><td>0.6230</td><td>0.4393</td><td>0.6458</td></tr>
            <tr><td>Comedy</td><td>0.5610</td><td>0.5793</td><td>0.6086</td></tr>
            <tr><td>Science Fiction</td><td>0.5352</td><td>0.3054</td><td>0.6004</td></tr>
            <tr><td>Action</td><td>0.4956</td><td>0.3933</td><td>0.5191</td></tr>
            <tr><td>Family</td><td>0.4596</td><td>0.4030</td><td>0.5151</td></tr>
            <tr><td>Crime</td><td>0.4851</td><td>0.2957</td><td>0.4821</td></tr>
            <tr><td>War</td><td>0.5270</td><td>0.1372</td><td>0.4811</td></tr>
            <tr><td>Thriller</td><td>0.4804</td><td>0.4062</td><td>0.4671</td></tr>
            <tr><td>Romance</td><td>0.4584</td><td>0.3937</td><td>0.3755</td></tr>
            <tr><td>Adventure</td><td>0.3508</td><td>0.2869</td><td>0.3407</td></tr>
            <tr><td>Fantasy</td><td>0.3892</td><td>0.2458</td><td>0.3353</td></tr>
            <tr><td>Mystery</td><td>0.2974 (Target: >0.25)</td><td>0.1700 (Target: >0.25)</td><td>0.3326 (Target: >0.25)</td></tr>
            <tr><td>History</td><td>0.3121 (Target: >0.20)</td><td>0.1439 (Target: >0.20)</td><td>0.2108 (Target: >0.20)</td></tr>
            <tr><td>TV Movie</td><td>0.1687 (Target: >0.20)</td><td>0.2342 (Target: >0.20)</td><td>0.1464 (Target: >0.20)</td></tr>
        </table>
        <p><img src="f1_scores_by_genre.png" alt="Text Model: F1 Scores by Genre"></p>
        <p><img src="f1_scores_image.png" alt="Image Model: F1 Scores by Genre"></p>
        <p><img src="f1_bar_plot.png" alt="Fusion Model: F1 Scores by Genre"></p>
        <p>The fusion model outperformed both single-modality models in most genres (e.g., Animation: 0.7169, Western: 0.7080), leveraging complementary textual and visual cues. The text-only model excelled for History (0.3121) and Music (0.7183), driven by semantic richness in synopses. The image-only model performed well for Animation (0.6636) and Comedy (0.5793) but struggled with rare genres like History (0.1439) and Mystery (0.1700) due to ambiguous visual features.</p>
        <h2>Visualizations</h2>
        <p><b>Text Model: Training and Validation Loss</b><br><img src="loss_plot.png" alt="Text Model Loss Curve"></p>
        <p><b>Image Model: Training and Validation Loss</b><br><img src="loss_curves_image.png" alt="Image Model Loss Curve"></p>
        <p><b>Fusion Model: Training and Validation Loss</b><br><img src="loss_curves_fusion.png" alt="Fusion Model Loss Curve"></p>
        <p><b>Fusion Model: Validation F1 Macro</b><br><img src="f1_scores_fusion.png" alt="Fusion Model F1 Macro Curve"></p>
        <p><b>Fusion Model: Validation Accuracy</b><br><img src="accuracy_scores_fusion.png" alt="Fusion Model Accuracy Curve"></p>
        <h2>Error Analysis</h2>
        <p><b>Fusion Model: History Confusion Matrix</b><br><img src="cm_history.png" alt="History Confusion Matrix"></p>
        <p><b>Fusion Model: TV Movie Confusion Matrix</b><br><img src="cm_tv_movie.png" alt="TV Movie Confusion Matrix"></p>
        <p>Misclassifications highlight genre overlap challenges. For the text-only model, <i>Destruction Force</i> (True: Crime) was predicted as Action, Crime, Drama, and Thriller due to its action-heavy synopsis. <i>The Perez Family</i> (True: Comedy, Drama, Romance) was misclassified as Documentary, likely due to historical context. The image-only model struggled with TV Movie (0.2342) and History (0.1439), as posters often resemble Drama or Family, leading to false negatives (see confusion matrices). The fusion model’s performance on TV Movie (0.1464) and History (0.2108) was limited by low sample counts ([Insert counts, e.g., 350, 420]). Class weights (2.0) improved History F1 from 0.1860 to 0.2108 and TV Movie from 0.1031 to 0.1464, but TV Movie fell short of the 0.20 target. Threshold tuning was conducted to optimize the decision boundary:</p>
    </div>
</body>
</html>
